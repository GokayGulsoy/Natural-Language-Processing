{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ],
      "metadata": {
        "id": "dvWLscmA-_5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# defining relu\n",
        "r = nn.ReLU()\n",
        "\n",
        "# defining Lrelu and the parameter 0.2 is passed to control the negative slope ; a=0.2\n",
        "lr = nn.LeakyReLU(0.2)\n",
        "\n",
        "# Calling the sigmoid function\n",
        "sig = nn.Sigmoid()\n",
        "\n",
        "# Calling the Tanh function\n",
        "t = nn.Tanh()\n",
        "\n",
        "# Calling the Softmax function with dimension = 0 as dimension starts from 0\n",
        "sm = nn.Softmax(dim=0)\n",
        "\n",
        "# Creating a Tensor with an array\n",
        "input = torch.Tensor([1, -2, 3, -5])\n",
        "\n",
        "# Passing the array to relu function\n",
        "output = r(input)  # r, lr, sig, t, sm\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "EA0lUO3fslXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Loss Functions from PyTorch"
      ],
      "metadata": {
        "id": "bOyhPOUnetj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 Loss:"
      ],
      "metadata": {
        "id": "Qf_tx1u3fZdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randn(3, 5)\n",
        "\n",
        "mae_loss = nn.L1Loss()\n",
        "output = mae_loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)"
      ],
      "metadata": {
        "id": "Gx2_XVHpetKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error Loss:"
      ],
      "metadata": {
        "id": "fF6EuSJjfbjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randn(3, 5)\n",
        "mse_loss = nn.MSELoss()\n",
        "output = mse_loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)"
      ],
      "metadata": {
        "id": "pLyUKnI_fAKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Log-Likelihood Loss:"
      ],
      "metadata": {
        "id": "7j6hIyXRfeqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size of input (N x C) is = 3 x 5\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "# every element in target should have 0 <= value < C\n",
        "target = torch.tensor([1, 0, 4])\n",
        "\n",
        "m = nn.LogSoftmax(dim=1)\n",
        "nll_loss = nn.NLLLoss()\n",
        "output = nll_loss(m(input), target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)"
      ],
      "metadata": {
        "id": "u3ly40vEfDI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Entropy Loss:"
      ],
      "metadata": {
        "id": "kX-0u_2Zfh8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "output = cross_entropy_loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)"
      ],
      "metadata": {
        "id": "lnKpgmW7fJQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional resources:**\n",
        "\n",
        "https://neptune.ai/blog/pytorch-loss-functions\n",
        "\n",
        "https://machinelearningmastery.com/loss-functions-in-pytorch-models/  (has a custom loss function example, might be useful)"
      ],
      "metadata": {
        "id": "uEfvsdxkgmNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Look at Backprop with PyTorch"
      ],
      "metadata": {
        "id": "0_nfeFnchGu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code section is only meant for showcasing model training steps with PyTorch library. Do not execute this code section.\n",
        "\n",
        "Additional Resource: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
      ],
      "metadata": {
        "id": "8b5j0wl5hqHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FORWARD PASS\n",
        "prediction = model(data)\n",
        "\n",
        "# BACKWARD PASS (BACKPROPAGATION)\n",
        "loss = (prediction - labels).sum()\n",
        "loss.backward()\n",
        "\n",
        "# OPTIMIZATION\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "# GRADIENT DESCENT\n",
        "optim.step()"
      ],
      "metadata": {
        "id": "myXNMkdihMII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Neural Network Model in Keras"
      ],
      "metadata": {
        "id": "pyD4uypUhqjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "K22-vRZmouXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I00UrUR2mBJe"
      },
      "outputs": [],
      "source": [
        "# first neural network with keras tutorial\n",
        "from numpy import loadtxt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# load the dataset\n",
        "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "\n",
        "# split into input (X) and output (y) variables\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "doiOfjwOiFns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sZ_N8AfKiNIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the keras model on the dataset\n",
        "model.fit(X, y, epochs=30, batch_size=10)"
      ],
      "metadata": {
        "id": "l5sS58HziP06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X, y)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "BNRX3I9hiVjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make probability predictions with the model\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions for first 5 predictions\n",
        "rounded = [round(x[0]) for x in predictions[:5]]\n",
        "\n",
        "print(\"\\n\", X[:5], \"\\n\")\n",
        "print(rounded)"
      ],
      "metadata": {
        "id": "2Rr5C_4ZppVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Example for Saving and Loading Models in PyTorch"
      ],
      "metadata": {
        "id": "3ESZ6bTOiZ4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data into NumPy arrays\n",
        "data = load_iris()\n",
        "X, y = data[\"data\"], data[\"target\"]\n",
        "\n",
        "# convert NumPy array into PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)"
      ],
      "metadata": {
        "id": "L4xngHl5q_MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch model\n",
        "class Multiclass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(4, 8)\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(8, 3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.hidden(x))\n",
        "        x = self.logsoftmax(self.output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "D_UwzsKIjIBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Multiclass()\n",
        "\n",
        "# loss metric and optimizer\n",
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "fk9_TliZjH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare model and training parameters\n",
        "n_epochs = 100\n",
        "batch_size = 5\n",
        "batch_start = torch.arange(0, len(X), batch_size)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(n_epochs):\n",
        "    for start in batch_start:\n",
        "        # take a batch\n",
        "        X_batch = X_train[start:start+batch_size]\n",
        "        y_batch = y_train[start:start+batch_size]\n",
        "        # forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()     #  <<<------------------- BACKPROP IS HERE!!!\n",
        "        loss.backward()\n",
        "        # update weights\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "wTYP87ujjHwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"iris-model.pth\")"
      ],
      "metadata": {
        "id": "BuR8RJQXjTvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data into NumPy arrays\n",
        "data = load_iris()\n",
        "X, y = data[\"data\"], data[\"target\"]\n",
        "\n",
        "# convert NumPy array into PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# PyTorch model\n",
        "class Multiclass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(4, 8)\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(8, 3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.hidden(x))\n",
        "        x = self.logsoftmax(self.output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "-K21fgm3rXsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new model and load states\n",
        "model = Multiclass()\n",
        "model.load_state_dict(torch.load(\"iris-model.pth\"))"
      ],
      "metadata": {
        "id": "KBJKEFAtjiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model for inference\n",
        "y_pred = model(X_test)\n",
        "acc = (torch.argmax(y_pred, 1) == y_test).float().mean()\n",
        "print(\"Accuracy: %.2f\" % acc)"
      ],
      "metadata": {
        "id": "wOdgC8Cojh_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Recurrent Neural Networks in Keras"
      ],
      "metadata": {
        "id": "kkshSXfbjvAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mN4Eync8wEMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter split_percent defines the ratio of training examples\n",
        "def get_train_test(url, split_percent=0.8):\n",
        "    df = read_csv(url, usecols=[1], engine='python')\n",
        "    data = np.array(df.values.astype('float32'))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data).flatten()\n",
        "    n = len(data)\n",
        "    # Point for splitting data into train and test\n",
        "    split = int(n*split_percent)\n",
        "    train_data = data[range(split)]\n",
        "    test_data = data[split:]\n",
        "    return train_data, test_data, data"
      ],
      "metadata": {
        "id": "nCmh75XQwBIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input X and target Y\n",
        "def get_XY(dat, time_steps):\n",
        "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
        "    Y = dat[Y_ind]\n",
        "    rows_x = len(Y)\n",
        "    X = dat[range(time_steps*rows_x)]\n",
        "    X = np.reshape(X, (rows_x, time_steps, 1))\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "uDn9qZ0b2PF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
        "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "At4ZE3pO2O_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_error(trainY, testY, train_predict, test_predict):\n",
        "    # Error of predictions\n",
        "    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n",
        "    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n",
        "    # Print RMSE\n",
        "    print('Train RMSE: %.3f RMSE' % (train_rmse))\n",
        "    print('Test RMSE: %.3f RMSE' % (test_rmse))"
      ],
      "metadata": {
        "id": "0kWz6exp2O31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the result\n",
        "def plot_result(trainY, testY, train_predict, test_predict):\n",
        "    actual = np.append(trainY, testY)\n",
        "    predictions = np.append(train_predict, test_predict)\n",
        "    rows = len(actual)\n",
        "    plt.figure(figsize=(15, 6), dpi=80)\n",
        "    plt.plot(range(rows), actual)\n",
        "    plt.plot(range(rows), predictions)\n",
        "    plt.axvline(x=len(trainY), color='r')\n",
        "    plt.legend(['Actual', 'Predictions'])\n",
        "    plt.xlabel('Observation number after given time steps')\n",
        "    plt.ylabel('Sunspots scaled')\n",
        "    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')"
      ],
      "metadata": {
        "id": "GY42m-l_2ea6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'\n",
        "time_steps = 12\n",
        "train_data, test_data, data = get_train_test(sunspots_url)\n",
        "trainX, trainY = get_XY(train_data, time_steps)\n",
        "testX, testY = get_XY(test_data, time_steps)"
      ],
      "metadata": {
        "id": "7FgtQqUT2igD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and train\n",
        "model = create_RNN(hidden_units=3, dense_units=1, input_shape=(time_steps,1),\n",
        "                   activation=['tanh', 'tanh'])\n",
        "model.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)"
      ],
      "metadata": {
        "id": "U2FoTEDS2lON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "train_predict = model.predict(trainX)\n",
        "test_predict = model.predict(testX)"
      ],
      "metadata": {
        "id": "C25oCiVr2rLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print error\n",
        "print_error(trainY, testY, train_predict, test_predict)"
      ],
      "metadata": {
        "id": "6kTeKP3K2uZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot result\n",
        "plot_result(trainY, testY, train_predict, test_predict)  # The red line separates the training and test data points."
      ],
      "metadata": {
        "id": "UXjPMDeU2wm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition with Transformers"
      ],
      "metadata": {
        "id": "2pMqcDdA3D3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1VaC-Zcgy6B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# create pipeline for NER\n",
        "ner = pipeline('ner', model=\"dslim/bert-base-NER\", aggregation_strategy = 'simple')\n",
        "\n",
        "ner(\"Hi, my name is James. I am from Pune. I want to work with Google.\")\n"
      ],
      "metadata": {
        "id": "c8jVgGFny-dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification with BERT"
      ],
      "metadata": {
        "id": "4V41L-1O1Iz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "v1dyrgiu1KgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # import files from drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "XeJxnAEp6WYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_imdb_data(data_file):\n",
        "    df = pd.read_csv(data_file)\n",
        "    texts = df['review'].tolist()\n",
        "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
        "    return texts, labels\n",
        "\n",
        "data_file = \"IMDB Dataset.csv\"  # /content/drive/MyDrive/Ders Asistanlıklarım/CENG464-Lab3/IMDB Dataset.csv\n",
        "texts, labels = load_imdb_data(data_file)"
      ],
      "metadata": {
        "id": "gL2EfChD1LpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "      self.texts = texts\n",
        "      self.labels = labels\n",
        "      self.tokenizer = tokenizer\n",
        "      self.max_length = max_length\n",
        "  def __len__(self):\n",
        "      return len(self.texts)\n",
        "  def __getitem__(self, idx):\n",
        "      text = self.texts[idx]\n",
        "      label = self.labels[idx]\n",
        "      encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "      return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "rP1kscWe154b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "RE15UZgf2Fa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ],
      "metadata": {
        "id": "Una667iN2Jk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "    return \"positive\" if preds.item() == 1 else \"negative\""
      ],
      "metadata": {
        "id": "VMfxZn9r2OeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up parameters\n",
        "bert_model_name = 'bert-base-uncased'  # 'google-t5/t5-small'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "qDEFSeV52RWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "VkK1Kas82dKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        train(model, train_dataloader, optimizer, scheduler, device)\n",
        "        accuracy, report = evaluate(model, val_dataloader, device)\n",
        "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "        print(report)"
      ],
      "metadata": {
        "id": "5F8g1LMU2oXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert_classifier.pth\")"
      ],
      "metadata": {
        "id": "fAsuTyx02ygY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentiment prediction\n",
        "test_text = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(\"The movie was great and I really enjoyed the performances of the actors.\")\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "1jtESJxb2z2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentiment prediction\n",
        "test_text = \"Worst movie of the year.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(\"Worst movie of the year.\")\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "hm0Grt1m228o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Additional Resources:**\n",
        "\n",
        "https://analyticsindiamag.com/top-ten-bert-alternatives-for-nlu-projects/\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "\n",
        "https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n",
        "\n",
        "https://medium.com/@lokaregns/named-entity-recognition-with-hugging-face-transformers-a-beginners-guide-e1ac6085fb3c\n",
        "\n",
        "https://huggingface.co/dslim/bert-base-NER\n",
        "\n",
        "https://medium.com/@hhpatil001/transformers-from-scratch-in-simple-python-part-i-b290760c1040\n",
        "\n",
        "https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/\n",
        "\n",
        "https://machinelearningmastery.com/save-and-load-your-pytorch-models/\n",
        "\n",
        "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
        "\n",
        "https://www.geeksforgeeks.org/activation-functions-in-pytorch/"
      ],
      "metadata": {
        "id": "avjRF0p_ibxc"
      }
    }
  ]
}